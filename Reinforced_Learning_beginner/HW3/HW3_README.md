# ДЗ №3 Методы семейства «актор-критик»

**Задача:** обучить агента действовать в средах с непрерывным пространством действий, используя алгоритм SAC (Soft Actor Critic) или PPO (Proximal Policy Optimization).

**Описание.** Ваша задача — обучить агента в одной из следующих сред: Half Cheetah, Walker2D, Humanoid, Inverted Double Pendulum. Эти среды доступны в библиотеках gym, pybullet-gym или DeepMind Control Suite. 

**Детали:**
* Рекомендуется использовать алгоритмы SAC или PPO для обучения.
* Проведите эксперименты с разными гиперпараметрами для достижения наилучших результатов.
* Пользуйтесь предоставленными ссылками и примерами для установки и использования необходимых библиотек.

**Что нужно сдать:**
* Код обучения в формате Jupyter.
* График сходимости, показывающий среднее вознаграждение.
* Веса обученной модели.
* Код для запуска модели с готовыми весами.
* Выводы по используемым гиперпараметрам.

<hr>

## Общий подход к решению задачи.

Существует несколько семейств алгоритмов для обучения агента в средах с непрерывным пространством действий. Для решения этой задачи я решила взять алгоритм SAC (Soft Actor Critic).
За основу реализации я взяла несколько репозиториев и статей:
- https://github.com/pranz24/pytorch-soft-actor-critic/tree/master
- https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/sac_continuous_action.py
- https://github.com/yosider/ml-agents-1/blob/master/docs/Training-SAC.md 
- Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement. Learning with a Stochastic Actor https://arxiv.org/pdf/1801.01290.pdf 
- Soft Actor-Critic Algorithms and Applications https://arxiv.org/pdf/1812.05905.pdf 

Как следует из названия алгоритм основан на взаимодействии актора и критика, которые максимизируют не только суммарное вознаграждение, но и энтропию. Основные части алгоритма включают:
- архитектура актор-критик, которые состоят из 2-х Q-function (в коде они названы critic и critic_target)
- off-policy алгоритм для использования набранного опыта, реализованный через replay buffer
- включение максимизации энтропии в policy function

С данным алгоритмом я проведу несколько экспериментов:
1.	Посмотрю влияние выбранной политики: сравню результаты алгоритма при стохастической vs детерминированной политиках. Если политика детерминирована, то энтропия не максимизирует и алгоритм напоминает DDPG, за исключением наличия 2-х Q-функций, hard target updates и использование фиксированного шума. За выбор политики в коде отвечает параметр POLICY.
2.	При выборе стохастической политики, максимизирующей энтропию вместе с вознаграждением, важную роль играет параметр температуры (в коде он назван ALPHA), который отвечает за определение важности максимизации вознаграждения vs энропии. Этот параметр требуется подбирать тщательно под задачу, поэтому авторы Soft Actor-Critic Algorithms and Applications (https://arxiv.org/pdf/1812.05905.pdf) предлагаю использовать методику автоматического подбора этого в процессе обучения. В коде за это отвечает параметр AUTOMATIC_ENTROPHY_TUNING

Итак, я проведу 3 эксперимента:
1.	**Эксперимент 1:** стохастическая политика + фиксированная температура (0,2 – этот параметр уже определен как самый эффективный для среды Mujoco Gym HalfCheetah-v2
2.	**Эксперимент 2:** cтохастическая политика + автоподбор температуры
3.	**Эксперимент 3:** детерминированная политика без эноропии
С остальными гиперпараметрами я не буду экспериментировать, детально они описаны в коде и я возьму оптимальные, которые уже получены исследователями.

## Результаты экспериментов:

> [!IMPORTANT]
> **Все тесты и инференс выполнялись на kaggle, так как у меня закончилась квота и не разворачивалсь среда mujoco!!!**

### Эксперимент 1.
В этом ноутбуке ([ссылка](https://www.kaggle.com/code/katyashakhova/exp1-cheetag-sac-inference)) обучение для **эксперимента 1**. Результаты обучения логируются в wandb и их можно посмотреть вот [здесь](https://wandb.ai/shakhova/RL_SAC?nw=nwuserkatya_shakhova). В этом же ноутбуке есть небольшой ролик, который показывает работу агента после обучения. Ниже приведен принт-скрин из wandb по обучению модели. Видно, что модель быстро набирает высокое вознаграждение за эпизод, на первых 100 тыс. шагах довольно неустойчива, затем корректирируется. В целом обучается довольно быстро.

![image](https://github.com/shakhovak/Study-projects-in-Uni/assets/89096305/b1418fdc-706c-4129-9c3c-272f415f210c)

По этой ссылке загруженые веса моделей после обучения (веса в виде модели из kaggle [ссылка на kaggle](https://www.kaggle.com/models/katyashakhova/exp1_cheetah_sac), те же веса, но уже в github - [ссылка на github](https://github.com/shakhovak/Study-projects-in-Uni/blob/master/Reinforced_Learning_beginner/HW3/Exp_1/sac_checkpoint_HalfCheetah-v2_v1))
В этом ноутбуке ([ссылка](https://www.kaggle.com/code/katyashakhova/exp1-cheetah-sac-inference)) я загружаю веса моделей и тестирую на одном эпизоде.

Ниже приведена gif по этому тесту, в данном тесте модель набрала более 10 000 за эпизод!

![](https://github.com/shakhovak/Study-projects-in-Uni/blob/master/Reinforced_Learning_beginner/HW3/ex1_sac.gif)


