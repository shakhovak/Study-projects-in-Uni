# ДЗ №3 Методы семейства «актор-критик»

**Задача:** обучить агента действовать в средах с непрерывным пространством действий, используя алгоритм SAC (Soft Actor Critic) или PPO (Proximal Policy Optimization).

**Описание.** Ваша задача — обучить агента в одной из следующих сред: Half Cheetah, Walker2D, Humanoid, Inverted Double Pendulum. Эти среды доступны в библиотеках gym, pybullet-gym или DeepMind Control Suite. 

**Детали:**
* Рекомендуется использовать алгоритмы SAC или PPO для обучения.
* Проведите эксперименты с разными гиперпараметрами для достижения наилучших результатов.
* Пользуйтесь предоставленными ссылками и примерами для установки и использования необходимых библиотек.

**Что нужно сдать:**
* Код обучения в формате Jupyter.
* График сходимости, показывающий среднее вознаграждение.
* Веса обученной модели.
* Код для запуска модели с готовыми весами.
* Выводы по используемым гиперпараметрам.

<hr>

## Общий подход к решению задачи.

Существует несколько семейств алгоритмов для обучения агента в средах с непрерывным пространством действий. Для решения этой задачи я решила взять алгоритм SAC (Soft Actor Critic).
За основу реализации я взяла несколько репозиториев и статей:
- https://github.com/pranz24/pytorch-soft-actor-critic/tree/master
- https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/sac_continuous_action.py
- https://github.com/yosider/ml-agents-1/blob/master/docs/Training-SAC.md 
- Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement. Learning with a Stochastic Actor https://arxiv.org/pdf/1801.01290.pdf 
- Soft Actor-Critic Algorithms and Applications https://arxiv.org/pdf/1812.05905.pdf 

Как следует из названия алгоритм основан на взаимодействии актора и критика, которые максимизируют не только суммарное вознаграждение, но и энтропию. Основные части алгоритма включают:
- архитектура актор-критик, которые состоят из 2-х Q-function (в коде они названы critic и critic_target)
- off-policy алгоритм для использования набранного опыта, реализованный через replay buffer
- включение максимизации энтропии в policy function

С данным алгоритмом я проведу несколько экспериментов:
1.	Посмотрю влияние выбранной политики: сравню результаты алгоритма при стохастической vs детерминированной политиках. Если политика детерминирована, то энтропия не максимизируется и алгоритм напоминает DDPG, за исключением наличия 2-х Q-функций, hard target updates и использование фиксированного шума. За выбор политики в коде отвечает параметр POLICY.
2.	При выборе стохастической политики, максимизирующей энтропию вместе с вознаграждением, важную роль играет параметр температуры (в коде он назван ALPHA), который отвечает за определение важности максимизации вознаграждения vs энропии. Этот параметр необходимо подбирать тщательно под задачу, поэтому авторы Soft Actor-Critic Algorithms and Applications (https://arxiv.org/pdf/1812.05905.pdf) предлагают использовать методику автоматического подбора этого в процессе обучения. В коде за это отвечает параметр AUTOMATIC_ENTROPHY_TUNING

Итак, я проведу 3 эксперимента:
1.	**Эксперимент 1:** стохастическая политика + фиксированная температура (0,2 – этот параметр уже определен как самый эффективный для среды Mujoco Gym HalfCheetah-v2).
2.	**Эксперимент 2:** cтохастическая политика + автоподбор температуры.
3.	**Эксперимент 3:** детерминированная политика без эноропии.
   
С остальными гиперпараметрами я не буду экспериментировать, детально они описаны в коде и я возьму оптимальные, которые уже получены исследователями.

Гиперпараметры, НЕ изменяемые в ходе экспериментов:
- **DEFAULT_ENV_NAME** - название среды из Mujoco Gym. Я буду проводить эксперименты только на одной среде, поэтому этот параметр не буду менять в рамках разных экспериментов.
- **GAMMA** - параметр, который определяет, какое внимание уделяет агент долгосрочным или краткосрочными наградами. В среде half-cheetah очевидно лучше краткосрочные награды, поэтому оставлю этот коэффициент равным 0,99.
- **START_STEPS** - кол-во шагов в начале обучения, в течение которых агент сэмплирует случайные данные для выбора действия из буфера действий. После этого агент возвращается к генерации действия на основе сети. Обычно берут значения от 1000 до 10 000. Я возьму максимальную величину оставлю 10 000, так как она лучше всего показала себя на средах Mujoco Gym.
- **TAU** - коэффициент для обновления обновления target сети (в коде critic target). Во всех экспериментах советуют брать величину, равную 0,005. Слишком большой коэффициент может привести к нестабильной работе агента, слишком маленький – замедлить обучение. На рисунке ниже исследование, проведенное авторами статьи «Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement. Learning with a Stochastic Actor» (https://arxiv.org/pdf/1801.01290.pdf). 
- **TARGET_UPDATE_INTERVAL** - в алгоритме SAC "update" представляет собой отбор из буфера опыта сэмпла размером batch_size и использование этого мини-батча для update моделей. Обычно данный коэффициент берется равным 1. 
- **CUDA** = True
- **LR** - learning rate
- **HIDDEN_SIZE**= 256 
- **SEED** - фиксируем случайность

Параметры только для моделей:
- **LOG_SIG_MAX**
- **LOG_SIG_MIN**
- **epsilon**

<hr>

## Результаты экспериментов:

> [!IMPORTANT]
> **Все тесты и инференс выполнялись на kaggle, так как у меня закончилась квота и не разворачивалсь среда mujoco!!!**

### Эксперимент 1 - стохастическая политика + фиксированная температура 0,2

Как я указывала раньше, для этого эксперимента я беру температуру 0.2, которая считается самой эффективной для SAC метода именно для этой среды. Для новых сред, конечно, этот параметр неизвестен сразу и нужно его подбирать.
В этом ноутбуке ([ссылка](https://www.kaggle.com/code/katyashakhova/exp1-cheetah-sac-train/notebook)) обучение для **эксперимента 1**. Результаты обучения логируются в wandb и их можно посмотреть вот [здесь](https://wandb.ai/shakhova/RL_SAC?nw=nwuserkatya_shakhova). В этом же ноутбуке есть небольшой ролик, который показывает работу агента после обучения. Ниже приведен принт-скрин из wandb по обучению модели. Видно, что модель быстро набирает высокое вознаграждение за эпизод, на первых 100 тыс. шагах довольно неустойчива, затем корректирируется. В целом обучается довольно быстро. Я запускала 2 run для таких параметров, оба получились довольно похожие несмотря на стохастичность среды, что говорит об устойчивости алгоритма.

![image](https://github.com/shakhovak/Study-projects-in-Uni/assets/89096305/9bc9b1ce-f136-4fe5-98a7-d3d904846c31)

Динамаика loss представлена на рисунке ниже. Интересно, что графики для обих run довольно похожие, только в том, где результаты немного хуже - есть большие выбросы:

![image](https://github.com/shakhovak/Study-projects-in-Uni/assets/89096305/b5d5d707-8a2c-4e9a-8929-f4884ac6bb81)


По этой ссылке загружены веса моделей после обучения (веса в виде модели из kaggle [ссылка на kaggle](https://www.kaggle.com/models/katyashakhova/exp1_cheetah_sac), те же веса, но уже в github - [ссылка на github](https://github.com/shakhovak/Study-projects-in-Uni/blob/master/Reinforced_Learning_beginner/HW3/Exp_1/sac_checkpoint_HalfCheetah-v2_v1))
В этом ноутбуке ([ссылка](https://www.kaggle.com/code/katyashakhova/exp1-cheetah-sac-inference)) я загружаю веса моделей и тестирую на одном эпизоде.

Ниже приведена gif по этому тесту, в данном тесте модель набрала более 10 000 за эпизод!

![](https://github.com/shakhovak/Study-projects-in-Uni/blob/master/Reinforced_Learning_beginner/HW3/ex1_sac.gif)

### Эксперимент 2 - стохастическая политика + подбор температуры

В данном эксперименте температура изначально не указывается, а модель обучается на подбор оптимальной величны.
В этом ноутбуке ([ссылка](https://www.kaggle.com/code/katyashakhova/exp2-cheetah-sac-train)) обучение для **эксперимента 2**. Результаты обучения логируются в wandb и их можно посмотреть вот [здесь](https://wandb.ai/shakhova/RL_SAC?nw=nwuserkatya_shakhova). В этом же ноутбуке есть небольшой ролик, который показывает работу агента после обучения. Ниже приведен принт-скрин из wandb по обучению модели. Запускалось обучение 2 раза, но один из run не дошел до конца (почему-то отключился kaggle :(). 

![image](https://github.com/shakhovak/Study-projects-in-Uni/assets/89096305/4bbd2bb1-20ca-49a2-8e5a-ce64fa4194ac)

Как видно из графика, модель при автоподборе температуры обучается медленнее, по сравнению с предыдущим вариантом, но тенденция к росту сохраняется. В конце обучения модель практически выучилась до оптимального уровня температуры. Однако данный коэффициент был все равно ниже оптимума, что частично объясняет более долгий процесс обучения.

Интересно посмотреть на динамику потерь при обучении на тепмературу (рис. ниже) - видно, что при приближении к оптимуму потери приближаются к нулю.

![image](https://github.com/shakhovak/Study-projects-in-Uni/assets/89096305/0e0503a5-67df-418b-b378-76f285a87a10)

В целом это приемлимый вариант при работе с незнакомыми средами, где заранее неизвестен оптимум для SAC алгоритма и требуется подобрать его вовремя обучения.

Веса модели для этого эксперимента сохранены как output в самом ноутбуке. Инференс по этому эксперименту не делала, так как результаты немного хуже, чем по эксперименту 1.

